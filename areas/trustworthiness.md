# Trustworthiness

* SecureBoost: A Lossless Federated Learning Framework [[Paper]](https://arxiv.org/pdf/1901.0)
* Fair Resource Allocation in Federated Learning [[Paper]](https://arxiv.org/abs/1905.10497)
* **How To Backdoor Federated Learning** [[Paper]](https://arxiv.org/abs/1807.00459) [[Github]](https://github.com/ebagdasa/
* Inverting Gradients - How easy is it to break privacy in federated learning? [[Paper]](https://papers.nips.cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf) [NIPS2020]

## Fairness

* Fair Resource Allocation in Federated Learning [[Paper]](https://arxiv.org/abs/1905.10497)

## Adversarial Attacks

* **How To Backdoor Federated Learning** [[Paper]](https://arxiv.org/abs/1807.00459) [[Github]](https://github.com/ebagdasa/backdoor_federated_learning)
* Can You Really Backdoor Federated Learning? [[Paper]](https://arxiv.org/abs/1911.07963)
* Model Poisoning Attacks in Federated Learning [[Paper]](https://dais-ita.org/sites/default/files/main_secml_model_poison.pdf) [NIPS workshop 2018]
* Inverting Gradients - How easy is it to break privacy in federated learning? [[Paper]](https://papers.nips.cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf) [NIPS2020]
* Attack of the Tails: Yes, You Really Can Backdoor Federated Learning [[Paper]](https://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf) [NIPS2020]
* Membership Inference Attacks Against Machine Learning Models [[Paper]](https://ieeexplore.ieee.org/abstract/document/7958568) [[Github]](https://github.com/csong27/membership-inference) [Cornell]

## Data Privacy and Confidentiality

* Election Coding for Distributed Learning: Protecting SignSGD against Byzantine Attacks [[Paper]](https://papers.nips.cc/paper/2020/file/a7f0d2b95c60161b3f3c82f764b1d1c9-Paper.pdf) [NIPS2020]
* Distributed Newton Can Communicate Less and Resist Byzantine Workers [[Paper]](https://arxiv.org/pdf/2006.08737.pdf) [Berkeley] [NIPS2020]
* Byzantine Resilient Distributed Multi-Task Learning [[Paper]](https://papers.nips.cc/paper/2020/file/d37eb50d868361ea729bb4147eb3c1d8-Paper.pdf) [NIPS2020]
* A Scalable Approach for Privacy-Preserving Collaborative Machine Learning [[Paper]](https://papers.nips.cc/paper/2020/file/5bf8aaef51c6e0d363cbe554acaf3f20-Paper.pdf) [USC] [NIPS2020]
* Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning [[Paper]](https://arxiv.org/abs/1805.05838) [NIPS 2019 Workshop]
* Quantification of the Leakage in Federated Learning [[Paper]](https://arxiv.org/pdf/1910.05467.pdf)
* Federated learning: distributed machine learning with data locality and privacy [[Blog]](https://blog.fastforwardlabs.com/2018/11/14/federated-learning.html)
* A Hybrid Approach to Privacy-Preserving Federated Learning [[Paper]](https://arxiv.org/abs/1812.03224)
* Analyzing Federated Learning through an Adversarial Lens [[Paper]](https://arxiv.org/pdf/1811.12470)
* Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attack [[Paper]](https://arxiv.org/abs/1812.00910)
* Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning [[Paper]](https://arxiv.org/pdf/1812.00535)
* Analyzing Federated Learning through an Adversarial Lens [[Paper]](https://arxiv.org/abs/1811.12470)
* Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning [[Paper]](https://arxiv.org/abs/1702.07464)
* Protection Against Reconstruction and Its Applications in Private Federated Learning [[Paper]](https://arxiv.org/pdf/1812.00984)
* Boosting Privately: Privacy-Preserving Federated Extreme Boosting for Mobile Crowdsensing [[Paper]](https://arxiv.org/abs/1907.10218)
* Privacy-Preserving Collaborative Deep Learning with Unreliable Participants [[Paper]](https://arxiv.org/abs/1812.10113)
* Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning [[Paper]](https://arxiv.org/pdf/1811.09904) [ICLR 2021]
* Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting [[Paper]](https://arxiv.org/pdf/1811.09712)
* Privacy-Preserving Deep Learning via Weight Transmission [[Paper]](https://arxiv.org/abs/1809.03272)
* Learning Private Neural Language Modeling with Attentive Aggregation [[Paper]](https://arxiv.org/pdf/1812.07108), IJCNN 2019 [[Code](https://github.com/shaoxiongji/fed-att)]
* Exploiting Unintended Feature Leakage in Collaborative Learning, an attack method related to membership inference [[Paper]](https://arxiv.org/abs/1805.04049) [[Github]](https://github.com/csong27/property-inference-collaborative-ml)


### Courses

* Applied Cryptography [[Udacity]](https://www.udacity.com/course/applied-cryptography--cs387)
  * Cryptography basics

### Differential Privacy

* A Brief Introduction to Differential Privacy [[Blog]](https://medium.com/georgian-impact-blog/a-brief-introduction-to-differential-privacy-eacf8722283b)
* *Deep Learning with Differential Privacy* [[Paper]](http://doi.acm.org/10.1145/2976749.2978318)
  * Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
* Differentially-Private Federated Linear Bandits [[Paper]](http://web.mit.edu/dubeya/www/files/dp_linucb_20.pdf) [[Slides]](http://web.mit.edu/dubeya/www/files/slides/nips20_fed.pdf) [MIT] [NIPS2020]
* Learning Differentially Private Recurrent Language Models [[Paper]](https://arxiv.org/abs/1710.06963)
* Federated Learning with Bayesian Differential Privacy [[Paper]](https://arxiv.org/abs/1911.10071) [NIPS 2019 Workshop]
* Private Federated Learning with Domain Adaptation [[Paper]](https://arxiv.org/abs/1912.06733) [NIPS 2019 Workshop]
* cpSGD: Communication-efficient and differentially-private distributed SGD [[Paper]](https://arxiv.org/abs/1805.10559)
* Federated Learning with Bayesian Differential Privacy [[Paper]](https://arxiv.org/pdf/1911.10071.pdf) [NIPS 2019 Workshop]
* Differentially Private Data Generative Models [[Paper]](https://arxiv.org/pdf/1812.02274)
* Differentially Private Federated Learning: A Client Level Perspective [[Paper]](https://arxiv.org/abs/1712.07557) [[Github]](https://github.com/SAP/machine-learning-diff-private-federated-learning) [ NIPS2017 Workshop]

#### PATE

* *Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data* [[Paper]](http://dblp.uni-trier.de/db/journals/corr/corr1610. )
  * Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar.
  * Private Aggregation of Teacher Ensembles (PATE)
* *Scalable Private Learning with PATE* [[Paper]](https://arxiv.org/abs/1802.08908)
  * Extension of PATE
- The [original PATE paper](https://arxiv.org/abs/1610.05755) at ICLR 2017 and recording of the ICLR [oral](https://www.youtube.com/watch?v=bDayquwDgjU)
- The [ICLR 2018 paper](https://arxiv.org/abs/1802.08908) on scaling PATE to large number of classes and imbalanced data.
- GitHub [code repo for PATE](https://github.com/tensorflow/models/tree/master/research/differential_privacy/multiple_teachers)
- GitHub [code repo for the refined privacy analysis of PATE](https://github.com/tensorflow/models/tree/master/research/differential_privacy/pate)



### Secure Multi-party Computation

#### Secret Sharing
* Simple Introduction to Sharmir's Secret Sharing and Lagrange Interpolation [[Youtube]](https://www.youtube.com/watch?v=kkMps3X_tEE)
* Secret Sharing, Part 1 [[Blog]](https://mortendahl.github.io/2017/06/04/secret-sharing-part1/): Shamir's Secret Sharing & Packed Variant
* Secret Sharing, Part 2 [[Blog]](https://mortendahl.github.io/2017/06/24/secret-sharing-part2/): Improve efficiency
* Secret Sharing, Part 3 [[Blog]](https://mortendahl.github.io/2017/08/13/secret-sharing-part3/)



#### SPDZ

* Basics of Secure Multiparty Computation [[Youtube]](https://www.youtube.com/watch?v=_mDlLKgiFDY): based on Shamir's Secret Sharing

* What is SPDZ?
  * Part 1: MPC Circuit Evaluation Overview [[Blog]](https://bristolcrypto.blogspot.com/2016/10/what-is-spdz-part-1-mpc-circuit.html)
  * Part 2: Circuit Evaluation [[Blog]](https://bristolcrypto.blogspot.com/2016/10/what-is-spdz-part-2-circuit-evaluation.html)

* The SPDZ Protocol [[Blog]](https://mortendahl.github.io/2017/09/03/the-spdz-protocol-part1/): implementation codes included



##### Advance (Not Recommended For Beginners)

* Multiparty Computation from Somewhat Homomorphic Encryption [[Paper]](https://eprint.iacr.org/2011/535)
  * SPDZ introduction
* Practical Covertly Secure MPC for Dishonest Majority – or: Breaking the SPDZ Limits [[Paper]](https://eprint.iacr.org/2012/642)
* MASCOT: Faster Malicious Arithmetic Secure Computation with Oblivious Transfer [[Paper]](https://eprint.iacr.org/2016/505)
* Removing the crypto provider and instead letting the parties generate these triples on their own
* Overdrive: Making SPDZ Great Again [[Paper]](https://eprint.iacr.org/2017/1230)
* Safetynets: Verifiable execution of deep neural networks on an untrusted cloud

#### Build Safe AI Series
* Building Safe A.I. [[Blog]](http://iamtrask.github.io/2017/03/17/safe-ai/)
  * A Tutorial for Encrypted Deep Learning
  * Use Homomorphic Encryption (HE)

* Private Deep Learning with MPC [[Blog]](https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/)
  * A Simple Tutorial from Scratch
  * Use Multiparty Compuation (MPC)

* Private Image Analysis with MPC [[Blog]](https://mortendahl.github.io/2017/09/19/private-image-analysis-with-mpc/)
  * Training CNNs on Sensitive Data
  * Use SPDZ as MPC protocol

#### MPC related Paper
Helen: Maliciously Secure Coopetitive Learning for Linear Models [[Paper]](https://arxiv.org/abs/1907.07212) [NIPS 2019 Workshop]



### Privacy Preserving Machine Learning

* Privacy-Preserving Deep Learning [[Paper]](https://www.comp.nus.edu.sg/~reza/files/Shokri-CCS2015.pdf)
* Privacy Partition: A Privacy-Preserving Framework for Deep Neural Networks in Edge Networks [[Paper]](http://mews.sv.cmu.edu/papers/archedge-18.pdf)
* *Practical Secure Aggregation for Privacy-Preserving Machine Learning* [[Paper]](https://eprint.iacr.org/2017/281.pdf) (Google)
  * Secure Aggregation: The problem of computing a multiparty sum where no party reveals its update in the clear—even to the aggregator
  * Goal: securely computing sums of vectors, which has a constant number of rounds, low communication overhead, robustness to failures, and which requires only one server with limited trust
  * Need to have basic knowledge of cryptographic algorithms such as secret sharing, key agreement, etc.
* *Practical Secure Aggregation for Federated Learning on User-Held Data* [[Paper]](https://arxiv.org/abs/1611.04482) (Google)
  * Highly related to *Practical Secure Aggregation for Privacy-Preserving Machine Learning*
  * Proposed 4 protocol one by one with gradual improvement to meet the requirement of secure aggregation propocol.
* SecureML: A System for Scalable Privacy-Preserving Machine Learning [[Paper]](https://eprint.iacr.org/2017/396.pdf)
* DeepSecure: Scalable Provably-Secure Deep Learning [[Paper]](https://arxiv.org/abs/1705.08963)
* Chameleon: A Hybrid Secure Computation Framework for Machine Learning Applications [[Paper]](https://arxiv.org/pdf/1801.03239.pdf)
* Contains several MPC frameworks